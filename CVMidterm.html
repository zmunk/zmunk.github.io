<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=eqLbnA_Efv82onanYUscTg');.lst-kix_hhnnp2o6wu1x-3>li:before{content:"-  "}.lst-kix_hhnnp2o6wu1x-4>li:before{content:"-  "}.lst-kix_mbq35qj9hsqb-0>li:before{content:"-  "}.lst-kix_hhnnp2o6wu1x-5>li:before{content:"-  "}.lst-kix_hhnnp2o6wu1x-1>li:before{content:"-  "}.lst-kix_mbq35qj9hsqb-2>li:before{content:"-  "}.lst-kix_hhnnp2o6wu1x-7>li:before{content:"-  "}ul.lst-kix_2hpuy1bprq1-8{list-style-type:none}.lst-kix_hhnnp2o6wu1x-0>li:before{content:"-  "}.lst-kix_mbq35qj9hsqb-3>li:before{content:"-  "}.lst-kix_mbq35qj9hsqb-4>li:before{content:"-  "}.lst-kix_mbq35qj9hsqb-5>li:before{content:"-  "}.lst-kix_hhnnp2o6wu1x-6>li:before{content:"-  "}ul.lst-kix_jwbte93xw3td-0{list-style-type:none}ul.lst-kix_jwbte93xw3td-4{list-style-type:none}ul.lst-kix_jwbte93xw3td-3{list-style-type:none}.lst-kix_hhnnp2o6wu1x-8>li:before{content:"-  "}ul.lst-kix_jwbte93xw3td-2{list-style-type:none}ul.lst-kix_jwbte93xw3td-1{list-style-type:none}ul.lst-kix_jwbte93xw3td-8{list-style-type:none}.lst-kix_kevuw7nat4u0-8>li:before{content:"\0025a0  "}ul.lst-kix_jwbte93xw3td-7{list-style-type:none}ul.lst-kix_jwbte93xw3td-6{list-style-type:none}ul.lst-kix_jwbte93xw3td-5{list-style-type:none}.lst-kix_kevuw7nat4u0-6>li:before{content:"\0025cf  "}.lst-kix_kevuw7nat4u0-7>li:before{content:"\0025cb  "}.lst-kix_mbq35qj9hsqb-1>li:before{content:"-  "}.lst-kix_ca89skfh28u5-2>li:before{content:"\0025a0  "}.lst-kix_kevuw7nat4u0-4>li:before{content:"\0025cb  "}.lst-kix_kevuw7nat4u0-5>li:before{content:"\0025a0  "}.lst-kix_ca89skfh28u5-4>li:before{content:"\0025cb  "}.lst-kix_ca89skfh28u5-0>li:before{content:"\0025cf  "}.lst-kix_ca89skfh28u5-3>li:before{content:"\0025cf  "}.lst-kix_muss065vu37-1>li{counter-increment:lst-ctn-kix_muss065vu37-1}.lst-kix_ca89skfh28u5-6>li:before{content:"\0025cf  "}.lst-kix_kevuw7nat4u0-1>li:before{content:"\0025cb  "}.lst-kix_kevuw7nat4u0-2>li:before{content:"\0025a0  "}.lst-kix_kevuw7nat4u0-3>li:before{content:"\0025cf  "}.lst-kix_ca89skfh28u5-5>li:before{content:"\0025a0  "}.lst-kix_s3183918wdsy-1>li{counter-increment:lst-ctn-kix_s3183918wdsy-1}.lst-kix_kevuw7nat4u0-0>li:before{content:"\0025cf  "}ol.lst-kix_s3183918wdsy-0.start{counter-reset:lst-ctn-kix_s3183918wdsy-0 0}.lst-kix_hhnnp2o6wu1x-2>li:before{content:"-  "}.lst-kix_ca89skfh28u5-1>li:before{content:"\0025cb  "}.lst-kix_41hc7a3xqlgz-8>li:before{content:"\0025a0  "}.lst-kix_41hc7a3xqlgz-7>li:before{content:"\0025cb  "}ul.lst-kix_41hc7a3xqlgz-1{list-style-type:none}ul.lst-kix_41hc7a3xqlgz-0{list-style-type:none}ol.lst-kix_muss065vu37-5.start{counter-reset:lst-ctn-kix_muss065vu37-5 0}ul.lst-kix_41hc7a3xqlgz-3{list-style-type:none}ul.lst-kix_41hc7a3xqlgz-2{list-style-type:none}ul.lst-kix_41hc7a3xqlgz-5{list-style-type:none}.lst-kix_41hc7a3xqlgz-0>li:before{content:"\0025cf  "}.lst-kix_41hc7a3xqlgz-1>li:before{content:"\0025cb  "}ul.lst-kix_41hc7a3xqlgz-4{list-style-type:none}ul.lst-kix_41hc7a3xqlgz-7{list-style-type:none}ul.lst-kix_41hc7a3xqlgz-6{list-style-type:none}ul.lst-kix_41hc7a3xqlgz-8{list-style-type:none}ul.lst-kix_2hpuy1bprq1-1{list-style-type:none}ul.lst-kix_2hpuy1bprq1-0{list-style-type:none}.lst-kix_41hc7a3xqlgz-4>li:before{content:"\0025cb  "}.lst-kix_41hc7a3xqlgz-5>li:before{content:"\0025a0  "}ul.lst-kix_2hpuy1bprq1-3{list-style-type:none}ul.lst-kix_2hpuy1bprq1-2{list-style-type:none}ul.lst-kix_2hpuy1bprq1-5{list-style-type:none}ol.lst-kix_s3183918wdsy-5.start{counter-reset:lst-ctn-kix_s3183918wdsy-5 0}ul.lst-kix_2hpuy1bprq1-4{list-style-type:none}.lst-kix_41hc7a3xqlgz-2>li:before{content:"\0025a0  "}.lst-kix_41hc7a3xqlgz-6>li:before{content:"\0025cf  "}ul.lst-kix_2hpuy1bprq1-7{list-style-type:none}ul.lst-kix_2hpuy1bprq1-6{list-style-type:none}.lst-kix_s3183918wdsy-5>li{counter-increment:lst-ctn-kix_s3183918wdsy-5}.lst-kix_s3183918wdsy-8>li{counter-increment:lst-ctn-kix_s3183918wdsy-8}.lst-kix_41hc7a3xqlgz-3>li:before{content:"\0025cf  "}.lst-kix_7mmhsfv5wlqq-1>li:before{content:"-  "}.lst-kix_7mmhsfv5wlqq-7>li:before{content:"-  "}.lst-kix_7mmhsfv5wlqq-5>li:before{content:"-  "}ul.lst-kix_mbq35qj9hsqb-6{list-style-type:none}.lst-kix_7mmhsfv5wlqq-3>li:before{content:"-  "}ul.lst-kix_mbq35qj9hsqb-5{list-style-type:none}ul.lst-kix_mbq35qj9hsqb-4{list-style-type:none}ul.lst-kix_mbq35qj9hsqb-3{list-style-type:none}ul.lst-kix_mbq35qj9hsqb-8{list-style-type:none}ul.lst-kix_mbq35qj9hsqb-7{list-style-type:none}.lst-kix_muss065vu37-4>li{counter-increment:lst-ctn-kix_muss065vu37-4}ol.lst-kix_s3183918wdsy-7.start{counter-reset:lst-ctn-kix_s3183918wdsy-7 0}ul.lst-kix_mbq35qj9hsqb-2{list-style-type:none}ul.lst-kix_mbq35qj9hsqb-1{list-style-type:none}ol.lst-kix_muss065vu37-3.start{counter-reset:lst-ctn-kix_muss065vu37-3 0}ul.lst-kix_mbq35qj9hsqb-0{list-style-type:none}.lst-kix_bzf39rp48c5-7>li:before{content:"\0025cb  "}.lst-kix_bzf39rp48c5-1>li:before{content:"\0025cb  "}ul.lst-kix_7mmhsfv5wlqq-4{list-style-type:none}ul.lst-kix_7mmhsfv5wlqq-3{list-style-type:none}ul.lst-kix_7mmhsfv5wlqq-2{list-style-type:none}ul.lst-kix_7mmhsfv5wlqq-1{list-style-type:none}ul.lst-kix_7mmhsfv5wlqq-8{list-style-type:none}ul.lst-kix_7mmhsfv5wlqq-7{list-style-type:none}ul.lst-kix_7mmhsfv5wlqq-6{list-style-type:none}ul.lst-kix_7mmhsfv5wlqq-5{list-style-type:none}.lst-kix_bzf39rp48c5-5>li:before{content:"\0025a0  "}.lst-kix_bzf39rp48c5-3>li:before{content:"\0025cf  "}ul.lst-kix_7mmhsfv5wlqq-0{list-style-type:none}ol.lst-kix_muss065vu37-0.start{counter-reset:lst-ctn-kix_muss065vu37-0 0}.lst-kix_up8f87x9hiuk-4>li:before{content:"\0025cb  "}.lst-kix_up8f87x9hiuk-8>li:before{content:"\0025a0  "}.lst-kix_up8f87x9hiuk-6>li:before{content:"\0025cf  "}.lst-kix_s3183918wdsy-4>li{counter-increment:lst-ctn-kix_s3183918wdsy-4}.lst-kix_ca89skfh28u5-7>li:before{content:"\0025cb  "}.lst-kix_up8f87x9hiuk-0>li:before{content:"\0025cf  "}ol.lst-kix_s3183918wdsy-8{list-style-type:none}ol.lst-kix_s3183918wdsy-6{list-style-type:none}ol.lst-kix_s3183918wdsy-7{list-style-type:none}ol.lst-kix_s3183918wdsy-4{list-style-type:none}ol.lst-kix_s3183918wdsy-5{list-style-type:none}ol.lst-kix_s3183918wdsy-2{list-style-type:none}ol.lst-kix_s3183918wdsy-3{list-style-type:none}.lst-kix_up8f87x9hiuk-2>li:before{content:"\0025a0  "}ol.lst-kix_s3183918wdsy-0{list-style-type:none}ol.lst-kix_s3183918wdsy-1{list-style-type:none}.lst-kix_fg2trobidd02-8>li:before{content:"-  "}.lst-kix_mbq35qj9hsqb-8>li:before{content:"-  "}.lst-kix_mbq35qj9hsqb-6>li:before{content:"-  "}.lst-kix_muss065vu37-5>li{counter-increment:lst-ctn-kix_muss065vu37-5}.lst-kix_nj0r7j4h9jqu-3>li:before{content:"\0025cf  "}ul.lst-kix_kevuw7nat4u0-1{list-style-type:none}ul.lst-kix_kevuw7nat4u0-0{list-style-type:none}ul.lst-kix_k90m6otahfkd-0{list-style-type:none}.lst-kix_2hpuy1bprq1-8>li:before{content:"-  "}.lst-kix_nj0r7j4h9jqu-2>li:before{content:"\0025a0  "}.lst-kix_nj0r7j4h9jqu-6>li:before{content:"\0025cf  "}.lst-kix_s3183918wdsy-2>li{counter-increment:lst-ctn-kix_s3183918wdsy-2}.lst-kix_s3183918wdsy-1>li:before{content:"" counter(lst-ctn-kix_s3183918wdsy-1,lower-roman) ") "}.lst-kix_nj0r7j4h9jqu-7>li:before{content:"\0025cb  "}ul.lst-kix_k90m6otahfkd-6{list-style-type:none}.lst-kix_2hpuy1bprq1-2>li:before{content:"-  "}.lst-kix_csnx2t6mi4dz-3>li:before{content:"\0025cf  "}ul.lst-kix_k90m6otahfkd-5{list-style-type:none}ul.lst-kix_up8f87x9hiuk-0{list-style-type:none}ul.lst-kix_kevuw7nat4u0-8{list-style-type:none}ul.lst-kix_k90m6otahfkd-8{list-style-type:none}ul.lst-kix_kevuw7nat4u0-7{list-style-type:none}ul.lst-kix_k90m6otahfkd-7{list-style-type:none}ul.lst-kix_kevuw7nat4u0-6{list-style-type:none}.lst-kix_s3183918wdsy-0>li:before{content:"" counter(lst-ctn-kix_s3183918wdsy-0,lower-latin) ") "}ul.lst-kix_k90m6otahfkd-2{list-style-type:none}ul.lst-kix_kevuw7nat4u0-5{list-style-type:none}.lst-kix_2hpuy1bprq1-1>li:before{content:"-  "}.lst-kix_csnx2t6mi4dz-2>li:before{content:"\0025a0  "}ul.lst-kix_k90m6otahfkd-1{list-style-type:none}ul.lst-kix_kevuw7nat4u0-4{list-style-type:none}ul.lst-kix_k90m6otahfkd-4{list-style-type:none}ul.lst-kix_kevuw7nat4u0-3{list-style-type:none}ul.lst-kix_k90m6otahfkd-3{list-style-type:none}ul.lst-kix_kevuw7nat4u0-2{list-style-type:none}.lst-kix_mfv57qacdb2q-0>li:before{content:"-  "}.lst-kix_csnx2t6mi4dz-7>li:before{content:"\0025cb  "}ol.lst-kix_muss065vu37-7.start{counter-reset:lst-ctn-kix_muss065vu37-7 0}.lst-kix_mfv57qacdb2q-1>li:before{content:"-  "}.lst-kix_mfv57qacdb2q-4>li:before{content:"-  "}.lst-kix_csnx2t6mi4dz-6>li:before{content:"\0025cf  "}.lst-kix_mfv57qacdb2q-8>li:before{content:"-  "}.lst-kix_fg2trobidd02-4>li:before{content:"-  "}.lst-kix_mfv57qacdb2q-5>li:before{content:"-  "}.lst-kix_fg2trobidd02-3>li:before{content:"-  "}.lst-kix_fg2trobidd02-0>li:before{content:"-  "}.lst-kix_o3r0171bkekx-1>li:before{content:"\0025cb  "}.lst-kix_jwbte93xw3td-5>li:before{content:"\0025a0  "}ul.lst-kix_fg2trobidd02-7{list-style-type:none}ul.lst-kix_fg2trobidd02-6{list-style-type:none}ul.lst-kix_fg2trobidd02-5{list-style-type:none}ol.lst-kix_muss065vu37-6.start{counter-reset:lst-ctn-kix_muss065vu37-6 0}.lst-kix_o3r0171bkekx-5>li:before{content:"\0025a0  "}ul.lst-kix_fg2trobidd02-4{list-style-type:none}ul.lst-kix_fg2trobidd02-3{list-style-type:none}ul.lst-kix_fg2trobidd02-2{list-style-type:none}.lst-kix_o3r0171bkekx-4>li:before{content:"\0025cb  "}.lst-kix_jwbte93xw3td-6>li:before{content:"\0025cf  "}ul.lst-kix_fg2trobidd02-1{list-style-type:none}ul.lst-kix_fg2trobidd02-0{list-style-type:none}.lst-kix_jwbte93xw3td-1>li:before{content:"\0025cb  "}ul.lst-kix_fg2trobidd02-8{list-style-type:none}.lst-kix_o3r0171bkekx-0>li:before{content:"\0025cf  "}.lst-kix_jwbte93xw3td-2>li:before{content:"\0025a0  "}ul.lst-kix_pw561kwiw4m0-5{list-style-type:none}ul.lst-kix_pw561kwiw4m0-6{list-style-type:none}ul.lst-kix_pw561kwiw4m0-7{list-style-type:none}ul.lst-kix_pw561kwiw4m0-8{list-style-type:none}ul.lst-kix_pw561kwiw4m0-0{list-style-type:none}ul.lst-kix_pw561kwiw4m0-1{list-style-type:none}ul.lst-kix_pw561kwiw4m0-2{list-style-type:none}ul.lst-kix_pw561kwiw4m0-3{list-style-type:none}ul.lst-kix_pw561kwiw4m0-4{list-style-type:none}ul.lst-kix_up8f87x9hiuk-7{list-style-type:none}ul.lst-kix_up8f87x9hiuk-8{list-style-type:none}ul.lst-kix_up8f87x9hiuk-5{list-style-type:none}ul.lst-kix_up8f87x9hiuk-6{list-style-type:none}ul.lst-kix_up8f87x9hiuk-3{list-style-type:none}ul.lst-kix_up8f87x9hiuk-4{list-style-type:none}ul.lst-kix_up8f87x9hiuk-1{list-style-type:none}ul.lst-kix_up8f87x9hiuk-2{list-style-type:none}.lst-kix_o3r0171bkekx-8>li:before{content:"\0025a0  "}.lst-kix_7mmhsfv5wlqq-2>li:before{content:"-  "}ul.lst-kix_csnx2t6mi4dz-1{list-style-type:none}ul.lst-kix_csnx2t6mi4dz-2{list-style-type:none}ul.lst-kix_csnx2t6mi4dz-3{list-style-type:none}ul.lst-kix_csnx2t6mi4dz-4{list-style-type:none}.lst-kix_7mmhsfv5wlqq-6>li:before{content:"-  "}.lst-kix_s3183918wdsy-7>li{counter-increment:lst-ctn-kix_s3183918wdsy-7}ul.lst-kix_csnx2t6mi4dz-0{list-style-type:none}ul.lst-kix_csnx2t6mi4dz-5{list-style-type:none}ul.lst-kix_csnx2t6mi4dz-6{list-style-type:none}ul.lst-kix_csnx2t6mi4dz-7{list-style-type:none}ul.lst-kix_csnx2t6mi4dz-8{list-style-type:none}.lst-kix_yp1d32cd33ka-2>li:before{content:"\0025a0  "}.lst-kix_bzf39rp48c5-6>li:before{content:"\0025cf  "}.lst-kix_bzf39rp48c5-2>li:before{content:"\0025a0  "}.lst-kix_srn8alsb0dhk-4>li:before{content:"-  "}.lst-kix_srn8alsb0dhk-0>li:before{content:"-  "}.lst-kix_up8f87x9hiuk-5>li:before{content:"\0025a0  "}ul.lst-kix_nj0r7j4h9jqu-8{list-style-type:none}ul.lst-kix_nj0r7j4h9jqu-7{list-style-type:none}.lst-kix_muss065vu37-8>li{counter-increment:lst-ctn-kix_muss065vu37-8}.lst-kix_w9ka78va5rsi-6>li:before{content:"\0025cf  "}.lst-kix_ca89skfh28u5-8>li:before{content:"\0025a0  "}.lst-kix_upn6eul5dnav-8>li:before{content:"-  "}.lst-kix_up8f87x9hiuk-1>li:before{content:"\0025cb  "}.lst-kix_srn8alsb0dhk-8>li:before{content:"-  "}.lst-kix_w9ka78va5rsi-2>li:before{content:"\0025a0  "}.lst-kix_s3183918wdsy-0>li{counter-increment:lst-ctn-kix_s3183918wdsy-0}.lst-kix_fg2trobidd02-7>li:before{content:"-  "}.lst-kix_s3183918wdsy-8>li:before{content:"" counter(lst-ctn-kix_s3183918wdsy-8,decimal) ". "}.lst-kix_muss065vu37-7>li:before{content:"" counter(lst-ctn-kix_muss065vu37-7,lower-latin) ". "}.lst-kix_mbq35qj9hsqb-7>li:before{content:"-  "}.lst-kix_s3183918wdsy-4>li:before{content:"(" counter(lst-ctn-kix_s3183918wdsy-4,lower-roman) ") "}ol.lst-kix_muss065vu37-8.start{counter-reset:lst-ctn-kix_muss065vu37-8 0}ul.lst-kix_nj0r7j4h9jqu-4{list-style-type:none}ul.lst-kix_nj0r7j4h9jqu-3{list-style-type:none}ul.lst-kix_nj0r7j4h9jqu-6{list-style-type:none}ul.lst-kix_nj0r7j4h9jqu-5{list-style-type:none}ul.lst-kix_nj0r7j4h9jqu-0{list-style-type:none}.lst-kix_2hpuy1bprq1-5>li:before{content:"-  "}ul.lst-kix_nj0r7j4h9jqu-2{list-style-type:none}ul.lst-kix_nj0r7j4h9jqu-1{list-style-type:none}.lst-kix_muss065vu37-3>li:before{content:"(" counter(lst-ctn-kix_muss065vu37-3,decimal) ") "}.lst-kix_k90m6otahfkd-7>li:before{content:"\0025cb  "}.lst-kix_k90m6otahfkd-8>li:before{content:"\0025a0  "}ol.lst-kix_muss065vu37-4.start{counter-reset:lst-ctn-kix_muss065vu37-4 0}.lst-kix_pw561kwiw4m0-1>li:before{content:"\0025cb  "}.lst-kix_k90m6otahfkd-5>li:before{content:"\0025a0  "}.lst-kix_k90m6otahfkd-6>li:before{content:"\0025cf  "}.lst-kix_9wijzkveb989-1>li:before{content:"-  "}.lst-kix_pw561kwiw4m0-0>li:before{content:"\0025cf  "}.lst-kix_pw561kwiw4m0-4>li:before{content:"\0025cb  "}.lst-kix_k90m6otahfkd-3>li:before{content:"\0025cf  "}.lst-kix_k90m6otahfkd-4>li:before{content:"\0025cb  "}ol.lst-kix_s3183918wdsy-6.start{counter-reset:lst-ctn-kix_s3183918wdsy-6 0}.lst-kix_pw561kwiw4m0-3>li:before{content:"\0025cf  "}.lst-kix_muss065vu37-0>li:before{content:"" counter(lst-ctn-kix_muss065vu37-0,decimal) ") "}.lst-kix_gp1ibqp1zuuw-2>li:before{content:"-  "}.lst-kix_9wijzkveb989-0>li:before{content:"-  "}.lst-kix_pw561kwiw4m0-2>li:before{content:"\0025a0  "}.lst-kix_gp1ibqp1zuuw-3>li:before{content:"-  "}.lst-kix_9wijzkveb989-6>li:before{content:"-  "}ul.lst-kix_srn8alsb0dhk-5{list-style-type:none}.lst-kix_k90m6otahfkd-0>li:before{content:"\0025cf  "}ul.lst-kix_srn8alsb0dhk-6{list-style-type:none}ul.lst-kix_srn8alsb0dhk-3{list-style-type:none}.lst-kix_pw561kwiw4m0-7>li:before{content:"\0025cb  "}ul.lst-kix_srn8alsb0dhk-4{list-style-type:none}.lst-kix_gp1ibqp1zuuw-4>li:before{content:"-  "}.lst-kix_gp1ibqp1zuuw-6>li:before{content:"-  "}ul.lst-kix_srn8alsb0dhk-1{list-style-type:none}.lst-kix_k90m6otahfkd-1>li:before{content:"\0025cb  "}.lst-kix_k90m6otahfkd-2>li:before{content:"\0025a0  "}ul.lst-kix_srn8alsb0dhk-2{list-style-type:none}.lst-kix_9wijzkveb989-5>li:before{content:"-  "}.lst-kix_pw561kwiw4m0-8>li:before{content:"\0025a0  "}ul.lst-kix_srn8alsb0dhk-0{list-style-type:none}.lst-kix_gp1ibqp1zuuw-5>li:before{content:"-  "}.lst-kix_9wijzkveb989-2>li:before{content:"-  "}.lst-kix_pw561kwiw4m0-5>li:before{content:"\0025a0  "}.lst-kix_gp1ibqp1zuuw-8>li:before{content:"-  "}.lst-kix_9wijzkveb989-4>li:before{content:"-  "}.lst-kix_9wijzkveb989-3>li:before{content:"-  "}.lst-kix_pw561kwiw4m0-6>li:before{content:"\0025cf  "}.lst-kix_gp1ibqp1zuuw-7>li:before{content:"-  "}.lst-kix_s3183918wdsy-3>li{counter-increment:lst-ctn-kix_s3183918wdsy-3}.lst-kix_upn6eul5dnav-0>li:before{content:"-  "}ul.lst-kix_o3r0171bkekx-0{list-style-type:none}.lst-kix_mbzrtzx2wb9v-8>li:before{content:"-  "}.lst-kix_upn6eul5dnav-5>li:before{content:"-  "}.lst-kix_upn6eul5dnav-6>li:before{content:"-  "}.lst-kix_mbzrtzx2wb9v-7>li:before{content:"-  "}.lst-kix_upn6eul5dnav-4>li:before{content:"-  "}.lst-kix_gp1ibqp1zuuw-1>li:before{content:"-  "}.lst-kix_upn6eul5dnav-3>li:before{content:"-  "}.lst-kix_gp1ibqp1zuuw-0>li:before{content:"-  "}.lst-kix_upn6eul5dnav-1>li:before{content:"-  "}.lst-kix_upn6eul5dnav-2>li:before{content:"-  "}.lst-kix_mbzrtzx2wb9v-0>li:before{content:"-  "}ul.lst-kix_gp1ibqp1zuuw-6{list-style-type:none}ul.lst-kix_gp1ibqp1zuuw-5{list-style-type:none}ul.lst-kix_gp1ibqp1zuuw-4{list-style-type:none}ul.lst-kix_gp1ibqp1zuuw-3{list-style-type:none}.lst-kix_muss065vu37-3>li{counter-increment:lst-ctn-kix_muss065vu37-3}ul.lst-kix_gp1ibqp1zuuw-8{list-style-type:none}ul.lst-kix_gp1ibqp1zuuw-7{list-style-type:none}.lst-kix_mbzrtzx2wb9v-4>li:before{content:"-  "}.lst-kix_mbzrtzx2wb9v-2>li:before{content:"-  "}.lst-kix_mbzrtzx2wb9v-6>li:before{content:"-  "}.lst-kix_yp1d32cd33ka-7>li:before{content:"\0025cb  "}ul.lst-kix_gp1ibqp1zuuw-2{list-style-type:none}ul.lst-kix_gp1ibqp1zuuw-1{list-style-type:none}.lst-kix_mbzrtzx2wb9v-1>li:before{content:"-  "}.lst-kix_mbzrtzx2wb9v-5>li:before{content:"-  "}.lst-kix_yp1d32cd33ka-6>li:before{content:"\0025cf  "}ul.lst-kix_gp1ibqp1zuuw-0{list-style-type:none}ul.lst-kix_o3r0171bkekx-2{list-style-type:none}ul.lst-kix_o3r0171bkekx-1{list-style-type:none}ul.lst-kix_o3r0171bkekx-4{list-style-type:none}ul.lst-kix_o3r0171bkekx-3{list-style-type:none}ul.lst-kix_o3r0171bkekx-6{list-style-type:none}ul.lst-kix_o3r0171bkekx-5{list-style-type:none}.lst-kix_mbzrtzx2wb9v-3>li:before{content:"-  "}.lst-kix_yp1d32cd33ka-8>li:before{content:"\0025a0  "}ul.lst-kix_o3r0171bkekx-8{list-style-type:none}ul.lst-kix_o3r0171bkekx-7{list-style-type:none}ul.lst-kix_mfv57qacdb2q-8{list-style-type:none}ul.lst-kix_mfv57qacdb2q-0{list-style-type:none}ul.lst-kix_mfv57qacdb2q-1{list-style-type:none}ul.lst-kix_mfv57qacdb2q-2{list-style-type:none}ul.lst-kix_mfv57qacdb2q-3{list-style-type:none}ul.lst-kix_mfv57qacdb2q-4{list-style-type:none}.lst-kix_9wijzkveb989-8>li:before{content:"-  "}ul.lst-kix_mfv57qacdb2q-5{list-style-type:none}ul.lst-kix_mfv57qacdb2q-6{list-style-type:none}.lst-kix_9wijzkveb989-7>li:before{content:"-  "}ul.lst-kix_mfv57qacdb2q-7{list-style-type:none}.lst-kix_yp1d32cd33ka-1>li:before{content:"\0025cb  "}.lst-kix_yp1d32cd33ka-3>li:before{content:"\0025cf  "}.lst-kix_yp1d32cd33ka-5>li:before{content:"\0025a0  "}.lst-kix_w9ka78va5rsi-7>li:before{content:"\0025cb  "}ul.lst-kix_upn6eul5dnav-3{list-style-type:none}ol.lst-kix_muss065vu37-1{list-style-type:none}ul.lst-kix_upn6eul5dnav-2{list-style-type:none}ol.lst-kix_muss065vu37-2{list-style-type:none}ul.lst-kix_upn6eul5dnav-5{list-style-type:none}ol.lst-kix_muss065vu37-3{list-style-type:none}ul.lst-kix_upn6eul5dnav-4{list-style-type:none}ol.lst-kix_muss065vu37-4{list-style-type:none}ol.lst-kix_muss065vu37-5{list-style-type:none}ol.lst-kix_muss065vu37-6{list-style-type:none}.lst-kix_muss065vu37-6>li{counter-increment:lst-ctn-kix_muss065vu37-6}ul.lst-kix_upn6eul5dnav-1{list-style-type:none}ol.lst-kix_muss065vu37-7{list-style-type:none}ul.lst-kix_upn6eul5dnav-0{list-style-type:none}ol.lst-kix_muss065vu37-8{list-style-type:none}.lst-kix_srn8alsb0dhk-3>li:before{content:"-  "}ol.lst-kix_muss065vu37-0{list-style-type:none}.lst-kix_srn8alsb0dhk-1>li:before{content:"-  "}ul.lst-kix_mbzrtzx2wb9v-2{list-style-type:none}ul.lst-kix_mbzrtzx2wb9v-3{list-style-type:none}ul.lst-kix_mbzrtzx2wb9v-0{list-style-type:none}ul.lst-kix_mbzrtzx2wb9v-1{list-style-type:none}.lst-kix_w9ka78va5rsi-1>li:before{content:"\0025cb  "}ul.lst-kix_mbzrtzx2wb9v-6{list-style-type:none}ul.lst-kix_mbzrtzx2wb9v-7{list-style-type:none}ul.lst-kix_mbzrtzx2wb9v-4{list-style-type:none}ul.lst-kix_mbzrtzx2wb9v-5{list-style-type:none}ul.lst-kix_upn6eul5dnav-7{list-style-type:none}ul.lst-kix_upn6eul5dnav-6{list-style-type:none}ul.lst-kix_mbzrtzx2wb9v-8{list-style-type:none}ul.lst-kix_upn6eul5dnav-8{list-style-type:none}ol.lst-kix_muss065vu37-2.start{counter-reset:lst-ctn-kix_muss065vu37-2 0}.lst-kix_srn8alsb0dhk-5>li:before{content:"-  "}.lst-kix_upn6eul5dnav-7>li:before{content:"-  "}.lst-kix_w9ka78va5rsi-5>li:before{content:"\0025a0  "}.lst-kix_srn8alsb0dhk-7>li:before{content:"-  "}.lst-kix_w9ka78va5rsi-3>li:before{content:"\0025cf  "}ul.lst-kix_hhnnp2o6wu1x-5{list-style-type:none}ul.lst-kix_hhnnp2o6wu1x-6{list-style-type:none}ul.lst-kix_hhnnp2o6wu1x-3{list-style-type:none}ul.lst-kix_hhnnp2o6wu1x-4{list-style-type:none}.lst-kix_muss065vu37-6>li:before{content:"" counter(lst-ctn-kix_muss065vu37-6,decimal) ". "}.lst-kix_muss065vu37-8>li:before{content:"" counter(lst-ctn-kix_muss065vu37-8,lower-roman) ". "}ul.lst-kix_hhnnp2o6wu1x-7{list-style-type:none}ul.lst-kix_srn8alsb0dhk-7{list-style-type:none}ul.lst-kix_hhnnp2o6wu1x-8{list-style-type:none}.lst-kix_s3183918wdsy-7>li:before{content:"" counter(lst-ctn-kix_s3183918wdsy-7,lower-roman) ". "}ul.lst-kix_srn8alsb0dhk-8{list-style-type:none}.lst-kix_muss065vu37-2>li:before{content:"" counter(lst-ctn-kix_muss065vu37-2,lower-roman) ") "}.lst-kix_muss065vu37-4>li:before{content:"(" counter(lst-ctn-kix_muss065vu37-4,lower-latin) ") "}ul.lst-kix_hhnnp2o6wu1x-1{list-style-type:none}ul.lst-kix_hhnnp2o6wu1x-2{list-style-type:none}ul.lst-kix_hhnnp2o6wu1x-0{list-style-type:none}.lst-kix_s3183918wdsy-5>li:before{content:"(" counter(lst-ctn-kix_s3183918wdsy-5,decimal) ") "}ol.lst-kix_s3183918wdsy-8.start{counter-reset:lst-ctn-kix_s3183918wdsy-8 0}.lst-kix_s3183918wdsy-3>li:before{content:"(" counter(lst-ctn-kix_s3183918wdsy-3,lower-latin) ") "}.lst-kix_2hpuy1bprq1-6>li:before{content:"-  "}.lst-kix_2hpuy1bprq1-7>li:before{content:"-  "}.lst-kix_csnx2t6mi4dz-0>li:before{content:"\0025cf  "}.lst-kix_2hpuy1bprq1-4>li:before{content:"-  "}.lst-kix_muss065vu37-0>li{counter-increment:lst-ctn-kix_muss065vu37-0}.lst-kix_2hpuy1bprq1-3>li:before{content:"-  "}.lst-kix_nj0r7j4h9jqu-0>li:before{content:"\0025cf  "}.lst-kix_nj0r7j4h9jqu-1>li:before{content:"\0025cb  "}.lst-kix_csnx2t6mi4dz-1>li:before{content:"\0025cb  "}.lst-kix_nj0r7j4h9jqu-8>li:before{content:"\0025a0  "}ul.lst-kix_9wijzkveb989-5{list-style-type:none}ol.lst-kix_muss065vu37-1.start{counter-reset:lst-ctn-kix_muss065vu37-1 0}.lst-kix_csnx2t6mi4dz-8>li:before{content:"\0025a0  "}ul.lst-kix_9wijzkveb989-4{list-style-type:none}ul.lst-kix_9wijzkveb989-3{list-style-type:none}ul.lst-kix_9wijzkveb989-2{list-style-type:none}.lst-kix_mfv57qacdb2q-2>li:before{content:"-  "}.lst-kix_2hpuy1bprq1-0>li:before{content:"-  "}ul.lst-kix_9wijzkveb989-8{list-style-type:none}ul.lst-kix_ca89skfh28u5-0{list-style-type:none}ul.lst-kix_9wijzkveb989-7{list-style-type:none}ul.lst-kix_ca89skfh28u5-1{list-style-type:none}ul.lst-kix_9wijzkveb989-6{list-style-type:none}.lst-kix_csnx2t6mi4dz-4>li:before{content:"\0025cb  "}.lst-kix_nj0r7j4h9jqu-5>li:before{content:"\0025a0  "}ul.lst-kix_9wijzkveb989-1{list-style-type:none}.lst-kix_csnx2t6mi4dz-5>li:before{content:"\0025a0  "}ul.lst-kix_9wijzkveb989-0{list-style-type:none}.lst-kix_mfv57qacdb2q-3>li:before{content:"-  "}.lst-kix_nj0r7j4h9jqu-4>li:before{content:"\0025cb  "}.lst-kix_fg2trobidd02-6>li:before{content:"-  "}.lst-kix_mfv57qacdb2q-6>li:before{content:"-  "}.lst-kix_fg2trobidd02-5>li:before{content:"-  "}.lst-kix_fg2trobidd02-2>li:before{content:"-  "}.lst-kix_mfv57qacdb2q-7>li:before{content:"-  "}.lst-kix_fg2trobidd02-1>li:before{content:"-  "}ol.lst-kix_s3183918wdsy-3.start{counter-reset:lst-ctn-kix_s3183918wdsy-3 0}ul.lst-kix_yp1d32cd33ka-0{list-style-type:none}.lst-kix_o3r0171bkekx-3>li:before{content:"\0025cf  "}.lst-kix_o3r0171bkekx-2>li:before{content:"\0025a0  "}.lst-kix_jwbte93xw3td-8>li:before{content:"\0025a0  "}ul.lst-kix_yp1d32cd33ka-1{list-style-type:none}ul.lst-kix_bzf39rp48c5-0{list-style-type:none}ul.lst-kix_yp1d32cd33ka-2{list-style-type:none}ul.lst-kix_yp1d32cd33ka-3{list-style-type:none}ul.lst-kix_yp1d32cd33ka-4{list-style-type:none}ul.lst-kix_yp1d32cd33ka-5{list-style-type:none}.lst-kix_jwbte93xw3td-7>li:before{content:"\0025cb  "}ul.lst-kix_yp1d32cd33ka-6{list-style-type:none}ul.lst-kix_yp1d32cd33ka-7{list-style-type:none}ul.lst-kix_yp1d32cd33ka-8{list-style-type:none}ul.lst-kix_bzf39rp48c5-8{list-style-type:none}ul.lst-kix_bzf39rp48c5-7{list-style-type:none}ul.lst-kix_bzf39rp48c5-6{list-style-type:none}ul.lst-kix_bzf39rp48c5-5{list-style-type:none}ul.lst-kix_bzf39rp48c5-4{list-style-type:none}ul.lst-kix_bzf39rp48c5-3{list-style-type:none}ul.lst-kix_bzf39rp48c5-2{list-style-type:none}.lst-kix_jwbte93xw3td-0>li:before{content:"\0025cf  "}.lst-kix_jwbte93xw3td-4>li:before{content:"\0025cb  "}ul.lst-kix_bzf39rp48c5-1{list-style-type:none}.lst-kix_jwbte93xw3td-3>li:before{content:"\0025cf  "}ol.lst-kix_s3183918wdsy-4.start{counter-reset:lst-ctn-kix_s3183918wdsy-4 0}.lst-kix_muss065vu37-7>li{counter-increment:lst-ctn-kix_muss065vu37-7}ul.lst-kix_ca89skfh28u5-2{list-style-type:none}ul.lst-kix_ca89skfh28u5-3{list-style-type:none}ul.lst-kix_ca89skfh28u5-4{list-style-type:none}ul.lst-kix_ca89skfh28u5-5{list-style-type:none}ul.lst-kix_ca89skfh28u5-6{list-style-type:none}ul.lst-kix_ca89skfh28u5-7{list-style-type:none}ul.lst-kix_ca89skfh28u5-8{list-style-type:none}.lst-kix_7mmhsfv5wlqq-8>li:before{content:"-  "}.lst-kix_o3r0171bkekx-7>li:before{content:"\0025cb  "}.lst-kix_o3r0171bkekx-6>li:before{content:"\0025cf  "}.lst-kix_7mmhsfv5wlqq-0>li:before{content:"-  "}.lst-kix_7mmhsfv5wlqq-4>li:before{content:"-  "}.lst-kix_bzf39rp48c5-8>li:before{content:"\0025a0  "}.lst-kix_s3183918wdsy-6>li{counter-increment:lst-ctn-kix_s3183918wdsy-6}.lst-kix_w9ka78va5rsi-8>li:before{content:"\0025a0  "}.lst-kix_yp1d32cd33ka-4>li:before{content:"\0025cb  "}.lst-kix_bzf39rp48c5-0>li:before{content:"\0025cf  "}ul.lst-kix_w9ka78va5rsi-1{list-style-type:none}ul.lst-kix_w9ka78va5rsi-0{list-style-type:none}ul.lst-kix_w9ka78va5rsi-3{list-style-type:none}ul.lst-kix_w9ka78va5rsi-2{list-style-type:none}ul.lst-kix_w9ka78va5rsi-5{list-style-type:none}.lst-kix_bzf39rp48c5-4>li:before{content:"\0025cb  "}ul.lst-kix_w9ka78va5rsi-4{list-style-type:none}.lst-kix_srn8alsb0dhk-2>li:before{content:"-  "}ul.lst-kix_w9ka78va5rsi-7{list-style-type:none}ul.lst-kix_w9ka78va5rsi-6{list-style-type:none}.lst-kix_yp1d32cd33ka-0>li:before{content:"\0025cf  "}ul.lst-kix_w9ka78va5rsi-8{list-style-type:none}ol.lst-kix_s3183918wdsy-1.start{counter-reset:lst-ctn-kix_s3183918wdsy-1 0}.lst-kix_w9ka78va5rsi-0>li:before{content:"\0025cf  "}.lst-kix_up8f87x9hiuk-7>li:before{content:"\0025cb  "}.lst-kix_w9ka78va5rsi-4>li:before{content:"\0025cb  "}.lst-kix_srn8alsb0dhk-6>li:before{content:"-  "}.lst-kix_up8f87x9hiuk-3>li:before{content:"\0025cf  "}ol.lst-kix_s3183918wdsy-2.start{counter-reset:lst-ctn-kix_s3183918wdsy-2 0}.lst-kix_s3183918wdsy-6>li:before{content:"" counter(lst-ctn-kix_s3183918wdsy-6,lower-latin) ". "}.lst-kix_muss065vu37-2>li{counter-increment:lst-ctn-kix_muss065vu37-2}.lst-kix_muss065vu37-1>li:before{content:"" counter(lst-ctn-kix_muss065vu37-1,lower-latin) ") "}.lst-kix_muss065vu37-5>li:before{content:"(" counter(lst-ctn-kix_muss065vu37-5,lower-roman) ") "}.lst-kix_s3183918wdsy-2>li:before{content:"" counter(lst-ctn-kix_s3183918wdsy-2,decimal) ") "}ol{margin:0;padding:0}table td,table th{padding:0}.c40{padding-top:0pt;text-indent:9pt;padding-bottom:10pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:center}.c10{margin-left:72pt;padding-top:0pt;padding-left:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c24{margin-left:27pt;padding-top:10pt;padding-bottom:3pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c8{margin-left:27pt;padding-top:12pt;padding-bottom:3pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c31{margin-left:27pt;padding-top:15pt;padding-bottom:3pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c26{margin-left:18pt;padding-top:12pt;padding-bottom:0pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c34{margin-left:18pt;padding-top:10pt;padding-bottom:0pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c28{padding-top:0pt;text-indent:9pt;padding-bottom:24pt;line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:right}.c35{margin-left:45pt;padding-top:18pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c1{margin-left:18pt;padding-top:0pt;padding-bottom:0pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c12{padding-top:0pt;text-indent:9pt;padding-bottom:10pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Georgia";font-style:normal}.c13{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:18pt;font-family:"Lora";font-style:normal}.c3{padding-top:0pt;text-indent:9pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c23{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:13pt;font-family:"Lora";font-style:normal}.c0{color:#666666;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Lora";font-style:normal}.c11{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Georgia";font-style:normal}.c25{padding-top:0pt;padding-left:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c16{padding-top:12pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c15{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:15pt;font-family:"Lora";font-style:normal}.c39{font-weight:700;text-decoration:none;vertical-align:baseline;font-size:24pt;font-family:"Lora";font-style:normal}.c33{color:#000000;font-weight:700;text-decoration:none;font-size:15pt;font-family:"Lora";font-style:normal}.c41{font-weight:400;text-decoration:none;vertical-align:baseline;font-size:18pt;font-family:"Georgia";font-style:normal}.c30{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c20{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-style:normal}.c5{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c29{color:#999999;text-decoration:none;vertical-align:baseline;font-style:italic}.c27{color:#666666;text-decoration:none;vertical-align:baseline;font-style:normal}.c21{font-weight:400;font-size:11pt;font-family:"Georgia"}.c18{vertical-align:sub;font-family:"Courier New";font-weight:400}.c14{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c2{padding:0;margin:0}.c9{color:inherit;text-decoration:inherit}.c32{font-style:italic;color:#999999}.c19{font-weight:400;font-family:"Courier New"}.c37{font-size:13.5pt}.c17{height:11pt}.c36{color:#000000}.c38{font-style:italic}.c7{margin-left:36pt}.c22{vertical-align:sub}.c6{vertical-align:super}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Georgia";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Georgia"}p{margin:0;color:#000000;font-size:11pt;font-family:"Georgia"}h1{padding-top:18pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:4pt;font-family:"Lora";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:12pt;color:#000000;font-weight:700;font-size:15pt;padding-bottom:3pt;font-family:"Lora";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:12pt;color:#000000;font-weight:700;font-size:13pt;padding-bottom:0pt;font-family:"Lora";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:0pt;color:#666666;font-weight:700;font-size:12pt;padding-bottom:0pt;font-family:"Lora";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c14"><p class="c40 title" id="h.af2d0hbx4i0d"><span class="c36 c39">Computer Vision Midterm Study Guide</span></p><p class="c28 subtitle"><span class="c21 c32">Ibrahim Tigrek</span></p><h1 class="c35" id="h.abbw2mu8xyt2"><span class="c13">Important Topics</span></h1><h2 class="c8" id="h.z7hkcegwwwg5"><span>Cross-Validation</span><span class="c5 c6"><a class="c9" href="https://www.google.com/url?q=https://www.google.com/url?q%3Dhttp://cs231n.github.io/classification/%26sa%3DD%26ust%3D1574278799830000&amp;sa=D&amp;ust=1574279330527000">2.2</a></span></h2><h4 class="c1" id="h.h8q0hqpuyym"><span class="c0">Purpose</span></h4><p class="c3"><span>The purpose of cross validation is t</span><span>o determine the optimum values of the hyperparameters, which are </span><span class="c4">the parameters that are set for the algorithm.</span></p><p class="c3"><span>k-</span><span>fold cross-validation is performed by dividing the training set into k sets, each one called a fold. Let&rsquo;s take k = 5, which is what is usually used. First, we train a model with a certain set of hyperparameters on the first 4 folds, and validate it (i.e. test it) on the 5th fold. Then we do the same, but this time the </span><span class="c38">validation set</span><span class="c4">&nbsp;is the 4th fold. Then again with the 3rd fold, and so on for all 5 possible combinations. For each of these iterations, we get a performance measure. We average these to get the final performance measure of the model with this specific set of hyperparameters.</span></p><p class="c3"><span class="c4">We do all of this again for another model, that has a different set of hyperparameters. And again for another model, and so on. Once we are satisfied with all the combinations of hyperparameters that we have validated, we choose the model that gave us the best overall performance measure. Then, and only then, do we use the test data, to test our final model.</span></p><p class="c3 c17"><span class="c4"></span></p><h2 class="c8" id="h.r9s32jnwg1t2"><span class="c15">Regularization</span></h2><p class="c3"><span>Regularization is a penalty term added to the loss function to keep the </span><span>weights</span><span class="c4">&nbsp;low. The strength of regularization depends on the hyperparameter &lambda;. Regularization makes the model simpler, and spreads out the weights.When building neural networks, it is recommended to start with as many neurons as the computer can handle, and apply regularization.</span></p><p class="c3"><span class="c4">The loss function with regularization looks like</span></p><p class="c3"><img src="images/image1.png"></p><p class="c3"><span>where </span><img src="images/image2.png"><span class="c4">&nbsp;is the regularization loss.</span></p><p class="c3"><span class="c4">Penalizing weight improve generalization suppose we have two weight vectors [1,0,0] and [</span></p><h4 class="c1" id="h.nlfknrw5d073"><span class="c0">L1 regularization</span></h4><p class="c3"><img src="images/image3.png"></p><p class="c3"><span class="c4">Also called Lasso regression. Lasso regression usually reduces useless features to zero.</span></p><h4 class="c1" id="h.bygdaau62vgc"><span class="c0">L2 regularization</span></h4><ul class="c2 lst-kix_9wijzkveb989-0 start"><li class="c25 c7"><span class="c4">R(W) summing up all the squared element of W </span></li></ul><p class="c3"><img src="images/image4.png"></p><p class="c3"><span>L2 regularization is a</span><span class="c4">lso called Ridge regression. This function is differentiable.</span></p><p class="c3 c17"><span class="c4"></span></p><h4 class="c1" id="h.7h2s7e9uoife"><span class="c0">Elastic net regularization</span></h4><p class="c3"><span class="c4">Is a combination of lasso and ridge regression. A hyperparameter (r) determines how much of lasso and ridge are used. </span></p><p class="c3"><img src="images/image5.png"></p><h4 class="c1" id="h.hv0e50arnb62"><span class="c0">Max norm constraints</span></h4><p class="c3"><span class="c4">Ensures that the norm of all of the weights is less than a specified bound. This is to prevent the neurons from becoming saturated.</span></p><h4 class="c1" id="h.r1tdi9pfjerc"><span class="c0">Dropout regularization</span></h4><p class="c3"><span class="c4">Starting with a fully-connected neural network, some neurons are killed off randomly based on a predefined probability. The &ldquo;killing&rdquo; of the neurons is done by cutting off all of the input to that neuron.</span></p><p class="c3"><span class="c4">Dropout regularization is used to weaken the strength of the network, thus making it harder for it to memorize the training data.</span></p><h4 class="c1" id="h.qvmzvt7fvtea"><span class="c0">Bias regularization</span></h4><p class="c3"><span class="c4">Bias does not require regularization. However, choosing to regularize the bias values will most probably not lead to worse performance.</span></p><h2 class="c8" id="h.441g8zed6ppz"><span class="c15">Gradient Descent</span></h2><p class="c3"><span class="c4">For every weight, the partial derivative of the loss function with respect to that weight is determined. Then, that weight is decreased by the product of the step size and the gradient (the partial derivative).</span></p><p class="c3"><img src="images/image6.png"></p><h4 class="c1" id="h.obubqvsecfde"><span class="c0">Mini-batch gradient descent</span></h4><p class="c3"><span class="c4">A small group of data points (e.g. 128) are used to determine the gradient.</span></p><h4 class="c1" id="h.dy6v9bqvexpv"><span class="c0">Stochastic gradient descent</span></h4><p class="c3"><span class="c4">The gradient is calculated and a step is taken for single data point. The data point in question is chosen randomly from the full dataset.</span></p><h4 class="c1" id="h.io8bkvjhlg0f"><span class="c0">Numerical gradient</span></h4><p class="c3"><span class="c4">Finding the gradient by taking two points that are close to each other and dividing the difference in y-values by the difference in their x-values. The numerical gradient can be computed for each case. It can be computed even if the original function is unknown analytically. </span></p><p class="c3"><span class="c4">If the distance between the x-values of the two points is h, and the smaller of the two is x, the numerical gradient can be computed as follows.</span></p><p class="c3"><img src="images/image7.png"></p><h4 class="c1" id="h.yrr4q5aupcr"><span class="c0">Analytic gradient</span></h4><p class="c3"><span class="c4">The formula for the exact gradient.</span></p><h2 class="c8" id="h.8e95tcb8x7px"><span>Back-propagation</span><span class="c5 c6"><a class="c9" href="https://www.google.com/url?q=https://www.google.com/url?q%3Dhttp://cs231n.github.io/optimization-2/%26sa%3DD%26ust%3D1574278799831000&amp;sa=D&amp;ust=1574279330540000">2.5</a></span></h2><h4 class="c1" id="h.a642lt558l5g"><span class="c0">The Algorithm</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 187.00px; height: 146.16px;"><img alt="" src="images/image17.png" style="width: 218.70px; height: 164.43px; margin-left: -31.70px; margin-top: -9.14px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></h4><p class="c3"><span>Each node is examined independently, where only the input values, the output value, and the gradient propagating back to this node are necessary. Let&rsquo;s call our node N, and the two inputs x and y. Let&rsquo;s call the function at this node f, where f produces the output of this node from x and y, i.e. the output is f(x, y). Finally, let&rsquo;s call the gradient that has propagated back to N from the next node, d. It is our job to compute the gradients d</span><span class="c22">x</span><span>&nbsp;and d</span><span class="c22">y</span><span>, which are the gradients that N will propagate back to the nodes before it, where d</span><span class="c22">x</span><span>&nbsp;will go back along the path that brought x to N, and d</span><span class="c22">y</span><span class="c4">&nbsp;similarly for y.</span></p><p class="c3"><span class="c19">d</span><span class="c18">x</span><span class="c20 c19">&nbsp;= d * f&rsquo;(x)</span></p><p class="c3"><span class="c19">d</span><span class="c18">y</span><span class="c20 c19">&nbsp;= d * f&rsquo;(y)</span></p><p class="c3"><span>where </span><span class="c19">f&rsquo; </span><span>is the partial derivative of f with respect to the argument. If, when computing </span><span class="c19">f&rsquo;(x)</span><span class="c4">, the function contains the variable y, then the value of the input in that specific case is used.</span></p><h4 class="c1" id="h.a9thrik9xptw"><span class="c0">Patterns</span></h4><p class="c3"><span class="c4">Some gates (nodes) have simple interpretations:</span></p><h5 class="c16" id="h.99dlv3tzqom9"><span class="c21 c27">Add gate</span></h5><p class="c3"><span class="c4">f(x, y) = x + y</span></p><p class="c3"><span class="c19">d</span><span class="c18">x</span><span class="c19 c20">&nbsp;= d</span></p><p class="c3"><span class="c19">d</span><span class="c18">y</span><span class="c20 c19">&nbsp;= d</span></p><h5 class="c16" id="h.2etqda1u6txk"><span class="c27 c21">Max gate</span></h5><p class="c3"><span class="c4">f(x, y) = max(x, y), assuming x &gt; y,</span></p><p class="c3"><span class="c19">d</span><span class="c18">x</span><span class="c20 c19">&nbsp;= d</span></p><p class="c3"><span class="c19">d</span><span class="c18">y</span><span class="c20 c19">&nbsp;= 0</span></p><h5 class="c16" id="h.2ysxt030sj3t"><span class="c27 c21">Multiply gate</span></h5><p class="c3"><span class="c4">f(x, y) = x * y</span></p><p class="c3"><span class="c19">d</span><span class="c18">x</span><span class="c20 c19">&nbsp;= d * y</span></p><p class="c3"><span class="c19">d</span><span class="c18">y</span><span class="c19">&nbsp;= d * x</span></p><h2 class="c8" id="h.460fkihq20vk"><span>Linear Classification</span><span class="c5 c21 c6"><a class="c9" href="https://www.google.com/url?q=https://www.google.com/url?q%3Dhttp://cs231n.github.io/classification/%26sa%3DD%26ust%3D1574278799830000&amp;sa=D&amp;ust=1574279330546000">2.2</a></span></h2><p class="c3"><span class="c4">There are no hidden layers in the neural network.</span></p><p class="c3"><span class="c4">- More powerful than KNN and it doesn&#39;t memorize the data it is less expensive in computation as it uses only one image calculation instead of comparing it with all the training images. </span></p><p class="c3"><span class="c4">- We will need the score function to map row data to class score and a loss function to calculate the difference between the predicted score and the truth labels &nbsp;.</span></p><p class="c3 c17"><span class="c4"></span></p><p class="c3"><span class="c4">- the score function :</span></p><p class="c3 c17"><span class="c4"></span></p><p class="c3 c7"><span class="c4">a) xi&isin;RD &nbsp;xi images with labels yi....N</span></p><p class="c3 c7"><span class="c4">b) yi&isin;1&hellip;K &nbsp; labels are element of range 1...K</span></p><p class="c3 c7"><span class="c4">c) if we have N examples of dimension &nbsp;and Categories K</span></p><p class="c3 c7"><span class="c4">d) &nbsp;the score function f:RD&#8614;RK</span></p><p class="c3 c7"><span class="c4">example CIFAR 10 K = 10 categories</span></p><p class="c3 c7"><span class="c4">N = 50K &nbsp;and D = 32*32*3 pixels</span></p><p class="c3 c17 c7"><span class="c4"></span></p><ul class="c2 lst-kix_2hpuy1bprq1-0 start"><li class="c7 c25"><span class="c4">Linear classifier : </span></li></ul><ol class="c2 lst-kix_s3183918wdsy-0 start" start="1"><li class="c10"><span class="c4">Flatten the images xi to the vector b[K*1] </span></li><li class="c10"><span class="c4">The matrix W[D*K] &nbsp;and the vector b[K*1] &nbsp;are the parameter </span></li><li class="c10"><span class="c4">f(xi,W,b) = Wxi + b</span></li><li class="c10"><span class="c4">W is the weight parameter and b is the bias vector </span></li><li class="c10"><span class="c4">(xi ,yi) &nbsp;are fixed but we have control over (W,b) </span></li><li class="c10"><span class="c4">Every row in W is a certain classifier. </span></li></ol><p class="c30 c17"><span class="c4"></span></p><h4 class="c1" id="h.wirufibgya9r"><span class="c0">Bias trick</span></h4><p class="c3"><span>Usually, the input vector x, to a neuron, is fed through the following formula: </span><span class="c19">y = Wx + b</span><span>, where W is a matrix; y and b are vectors. But the bias trick allows us to use only one matrix without adding a bias separately; by concatenating W with b which creates </span><span class="c19">W&rsquo;</span><span>and adding 1 to the bottom of the x vector, creating</span><span class="c19">&nbsp;x&rsquo;.</span><span>&nbsp;So, we get the same y as earlier by</span><span class="c19">&nbsp;y = W&rsquo; * x&rsquo;.</span></p><h4 class="c1" id="h.9vn6elcnwkqb"><span class="c0">Softmax classifier</span></h4><p class="c3"><span class="c4">(Multinomial Logistic Regression)</span></p><p class="c3"><span>This classifier gives the probability of each class label based on the softmax function. (see </span><span class="c5"><a class="c9" href="#h.ng115ad76ez">softmax loss function</a></span><span class="c4">)</span></p><p class="c3"><span class="c4">When the number of labels is really large, a variation of softmax, called &ldquo;hierarchical softmax&rdquo;, is used, since it is faster to evaluate.</span></p><h1 class="c35" id="h.vj9f20camcvs"><span class="c13">Related Topics</span></h1><h2 class="c8" id="h.xlehv8hw21ad"><span class="c15">Perceptron</span></h2><p class="c12"><span>A</span><span class="c21 c36">&nbsp;network consisting of inputs feeding into a single neuron </span><span>which has </span><span class="c21 c36">a step function as its activation function. Single p</span><span>erceptron wasn&rsquo;t enough to solve the XOR-problem. However, creating a hidden layer with two perceptrons can solve the problem.</span></p><h2 class="c8" id="h.u3l6b5nyhh14"><span class="c15">Activation functions</span></h2><h4 class="c1" id="h.34edzig8hemd"><span class="c0">step function</span></h4><h4 class="c1" id="h.34edzig8hemd-1"><span class="c0">sigmoid function</span></h4><h4 class="c1" id="h.34edzig8hemd-2"><span class="c0">hyperbolic tangent function</span></h4><h4 class="c1" id="h.75wcoj949i5w"><span class="c0">relu (rectified linear unit) function</span></h4><p class="c3"><span class="c4">Most commonly used in neural networks.</span></p><h2 class="c8" id="h.i98ollver278"><span class="c15">Loss functions</span></h2><p class="c3"><span>Consist of two parts: the error and the regularization. (see </span><span class="c5"><a class="c9" href="#h.r9s32jnwg1t2">Regularization</a></span><span class="c4">)</span></p><h4 class="c1" id="h.y0ptdaf1jgin"><span class="c0">Multiclass SVM loss</span></h4><p class="c3"><span>(S</span><span class="c22">j</span><span>) is the score of </span><span>j, and the &nbsp;</span><span>S</span><span class="c22">yi</span><span>&nbsp;is the true score of label y , and </span><img src="images/image8.png"><span class="c4">&nbsp;is the sum of the incorrect classes </span></p><p class="c3"><img src="images/image9.png"><span class="c4">&nbsp; &nbsp; Note: safety margin here is 1.</span></p><p class="c3 c17"><span class="c4"></span></p><ul class="c2 lst-kix_fg2trobidd02-0 start"><li class="c25 c7"><span>If &nbsp;y</span><span class="c22">i </span><span class="c4">= 0 means the class was correctly labeled &nbsp;the first part of the equation will be 0 and the second part will be positive number which due to the safety margin in this case = 1, in short svm loss wants the correct labels to be higher in the score with at least the safety margin. </span></li></ul><h4 class="c1" id="h.g3foklfps5wm"><span class="c0">Hinge loss</span></h4><p class="c3"><img src="images/image10.png"><span class="c4">&nbsp; where t=1, if 1 is the intended output. </span></p><p class="c3"><span>And </span><img><span class="c4">&nbsp;is the classifier score. </span></p><p class="c3"><span>The hinge los is the threshold at zero </span><span class="c38">max</span><span>(0,&minus;),</span><span class="c37">&nbsp;</span><span>in some dataset squared hinge loss work better &nbsp;max(0,- )</span><span class="c6">2 </span><span>&nbsp;however the unsquared version is more standard. </span></p><h4 class="c1" id="h.ng115ad76ez"><span class="c0">Softmax loss function</span></h4><p class="c3 c17"><span class="c4"></span></p><p class="c3"><img src="images/image11.png"></p><p class="c3"><span>where </span><img src="images/image12.png"></p><p class="c3 c17"><span class="c4"></span></p><p class="c3 c17"><span class="c4"></span></p><h4 class="c1" id="h.mhz5ssbcueo5"><span class="c0">Log loss function</span></h4><p class="c3"><img src="images/image13.png"></p><p class="c3"><span>where </span><img><span class="c4">&nbsp;is the output of the prediction fed through the sigmoid activation function.</span></p><h4 class="c1" id="h.jsc83bjkdul7"><span class="c0">Cross entropy loss function</span></h4><p class="c3"><img src="images/image14.png"></p><h2 class="c8" id="h.qdngtzlobw9f"><span class="c15">Nearest neighbor classifier</span></h2><p class="c3"><span>Memorizes all data and labels. Predicts the label of the closest data point. (see </span><span class="c5"><a class="c9" href="#h.qt9qbjyo8ex0">L1 and L2 distance</a></span><span class="c4">)</span></p><h2 class="c8" id="h.qt9qbjyo8ex0"><span>k-nearest neighbor classifier </span><span class="c5 c6"><a class="c9" href="https://www.google.com/url?q=https://www.google.com/url?q%3Dhttp://cs231n.github.io/classification/%26sa%3DD%26ust%3D1574278799830000&amp;sa=D&amp;ust=1574279330573000">2</a></span><span class="c5 c6"><a class="c9" href="https://www.google.com/url?q=https://www.google.com/url?q%3Dhttp://cs231n.github.io/classification/%26sa%3DD%26ust%3D1574278799830000&amp;sa=D&amp;ust=1574279330574000">.2</a></span></h2><p class="c3"><span class="c4">KNN summary : - in image classification we start with the training set and labels and must predict labels on test sets</span></p><p class="c3"><span class="c4">- the KNN classifier predict labels based on the nearest neighbors:</span></p><p class="c3"><span class="c4">- distance metrics and K are the hyper-parameters choose this in the validation set.</span></p><p class="c3"><span class="c4">Two different distances</span></p><ol class="c2 lst-kix_muss065vu37-0 start" start="1"><li class="c25 c7"><span>Manhattan distance : </span><img src="images/image15.png"></li><li class="c25 c7"><span>Euclidean distance : </span><img src="images/image16.png"></li></ol><p class="c3"><span class="c4">Predicts based on classes of majority of labels of k nearest neighbors. Odd number is used for k to prevent stalemate vote.</span></p><p class="c3"><span>This classifier does no work to train.</span></p><h2 class="c8" id="h.ljjiaff9bjy6"><span class="c15">Approximate nearest neighbor</span></h2><p class="c3"><span class="c4">Gives a neighbor that is very likely to be the nearest neighbor but doesn&rsquo;t guarantee it.</span></p><h2 class="c8" id="h.o0pqsn0uu7d"><span class="c15">Viewpoints</span></h2><ul class="c2 lst-kix_hhnnp2o6wu1x-0 start"><li class="c25 c7"><span class="c4">algebraic</span></li><li class="c25 c7"><span class="c4">visual</span></li><li class="c25 c7"><span class="c4">geometric</span></li></ul><h2 class="c31" id="h.s6d5od5pkzlu"><span class="c15">Optimization</span></h2><h4 class="c1" id="h.c5re9lng8m7i"><span class="c0">Strategies</span></h4><p class="c3"><span>r</span><span class="c4">andom search</span></p><p class="c3"><span class="c4">random local search</span></p><p class="c3"><span>following the gradient (see </span><span class="c5"><a class="c9" href="#h.441g8zed6ppz">gradient descent</a></span><span class="c4">)</span></p><h2 class="c8" id="h.ekdtc5tubypw"><span class="c15">Activation functions</span></h2><p class="c3"><span class="c4">sigmoid</span></p><p class="c3"><span class="c4">relu</span></p><p class="c3"><span class="c4">hyperbolic tangent</span></p><p class="c3"><span class="c4">leaky relu</span></p><p class="c3"><span>maxout</span></p><h2 class="c8" id="h.8x454o2fohmq"><span class="c15">Single neuron classifiers</span></h2><p class="c3"><span class="c4">Binary softmax classifier</span></p><p class="c12"><span class="c4">Binary SVM classifier</span></p><h2 class="c24" id="h.5k3i3zsc20uw"><span class="c15">Data preprocessing</span></h2><p class="c3"><span class="c4">mean subtraction</span></p><p class="c3"><span class="c4">normalization</span></p><p class="c3"><span class="c4">PCA (principal component analysis)</span></p><h2 class="c24" id="h.n8u481e84pk6"><span class="c15">Weight initialization</span></h2><h4 class="c1" id="h.8yjwux8cr6b5"><span class="c0">all zero initialization</span></h4><p class="c3"><span class="c4">All the weights are initialized to zero. The downside is that this will cause all of them to undergo the same parameter updates. This means there is no asymmetry among the weights, making the network struggle to learn.</span></p><h4 class="c1" id="h.2c5acm7or6c7"><span class="c0">small random numbers</span></h4><p class="c3"><span class="c4">The weights of the neurons are all random and unique in the beginning, so they will compute distinct updates and integrate themselves as diverse parts of the full network. A problem is that the variance grows with the number of inputs.</span></p><h4 class="c1" id="h.lnyd2mdsw9h1"><span class="c0">calibrating variances</span></h4><p class="c3"><span class="c4">Compensates for the increasing variance by scaling each neurons weight vector by the square root of the number of inputs to that neuron.</span></p><h4 class="c1" id="h.wc2cxirs0g83"><span class="c0">sparse initialization</span></h4><p class="c3"><span>Weight matrices initialized as zero, but each neuron is randomly connected to a fixed number of neurons, </span><span>not to all</span><span class="c4">&nbsp;of them.</span></p><h2 class="c8" id="h.l6e9b9ing0hj"><span class="c15">Bias initialization</span></h2><p class="c3"><span class="c4">Zero-bias initialization works fine.</span></p><h2 class="c8" id="h.1mxrye7qj82z"><span class="c15">Batch normalization</span></h2><p class="c3"><span>Normalizing the output of a </span><span>layer</span><span class="c4">&nbsp;before passing it to the next layer. This is to ensure that the neurons don&rsquo;t get saturated.</span></p><h2 class="c31" id="h.dtp51kmgxf13"><span class="c15">Overfitting and Underfitting</span></h2><p class="c3"><span class="c4">The desired model is the one that is neither high bias nor high variance.</span></p><h4 class="c1" id="h.ezjt1trs5k1b"><span class="c0">High-bias</span></h4><p class="c3"><span class="c4">A model that is too simple is considered to have high bias and low variance. It is also called under-fitting.</span></p><h4 class="c1" id="h.lzb4ugqwfjyz"><span class="c0">High-variance</span></h4><p class="c3"><span class="c4">A model that is too complex is considered to have high variance and low bias. It is also called over-fitting.</span></p><h2 class="c8" id="h.53sil47c1hok"><span class="c15">Saturation of a neuron</span></h2><p class="c3"><span class="c4">This occurs when the output of a neuron is insensitive to the input, i.e. it is the same regardless of any changes to the input. This is considered a dead, or dying, neuron. This happens when the input value has become very large or very small (negative).</span></p><h2 class="c8" id="h.dt7nchdtvn2a"><span class="c15">Additional topics</span></h2><p class="c3"><span class="c4">binary cross entropy</span></p><p class="c3"><span class="c4">L2 regression loss function</span></p><p class="c3"><span class="c4">Momentum optimization</span></p><p class="c3"><span class="c4">Stratified sampling</span></p><p class="c3"><span class="c4">Vanishing gradient problem</span></p><p class="c3"><span class="c4">SIFT (scale-invariant feature transform)</span></p><p class="c3"><span class="c4">One-shot learning</span></p><h1 class="c35" id="h.7jwdpdkaxu4t"><span class="c13">Appendix</span></h1><h2 class="c8" id="h.se5mth9nayk6"><span class="c15">Main resources</span></h2><h3 class="c26" id="h.j5ase9f9gh05"><span class="c23">Packt textbook Ch. 1 </span></h3><p class="c12"><span class="c5 c21"><a class="c9" href="https://www.google.com/url?q=https://subscription.packtpub.com/book/data/9781788830645/1/ch01lvl1sec02/computer-vision-in-the-wild&amp;sa=D&amp;ust=1574279330591000">https://subscription.packtpub.com/book/data/9781788830645/1/ch01lvl1sec02/computer-vision-in-the-wild</a></span></p><h3 class="c26" id="h.d3xip9nt8msq"><span class="c23">Stanford Neural Networks</span></h3><h4 class="c1" id="h.o9b4myvpyvxc"><span class="c0">Main page</span></h4><p class="c3"><span class="c6">2.1</span><span>&nbsp;</span><span class="c5"><a class="c9" href="https://www.google.com/url?q=http://cs231n.github.io/&amp;sa=D&amp;ust=1574279330592000">http://cs231n.github.io/</a></span></p><h4 class="c1" id="h.xj8p7u56q44h"><span class="c0">Classification</span></h4><p class="c3"><span class="c6">2.2</span><span>&nbsp;</span><span class="c5"><a class="c9" href="https://www.google.com/url?q=http://cs231n.github.io/classification/&amp;sa=D&amp;ust=1574279330596000">http://cs231n.github.io/classification/</a></span></p><h4 class="c1" id="h.cxtmt6uu70eb"><span class="c0">Linear classification</span></h4><p class="c3"><span class="c6">2.3 </span><span class="c5"><a class="c9" href="https://www.google.com/url?q=http://cs231n.github.io/linear-classify/&amp;sa=D&amp;ust=1574279330597000">http://cs231n.github.io/linear-classify/</a></span></p><h4 class="c1" id="h.og6hu97zmjo"><span class="c0">Optimization</span></h4><p class="c3"><span class="c6">2.4 </span><span class="c5"><a class="c9" href="https://www.google.com/url?q=http://cs231n.github.io/optimization-1/&amp;sa=D&amp;ust=1574279330597000">http://cs231n.github.io/optimization-1/</a></span></p><h4 class="c1" id="h.a0bztbttu3gy"><span class="c0">Back-propagation</span></h4><p class="c3"><span class="c6">2.5 </span><span class="c5"><a class="c9" href="https://www.google.com/url?q=http://cs231n.github.io/optimization-2/&amp;sa=D&amp;ust=1574279330598000">http://cs231n.github.io/optimization-2/</a></span></p><h4 class="c1" id="h.n765s5jmkrq2"><span class="c0">Neural networks part 1</span></h4><p class="c3"><span class="c6">2.6 </span><span class="c5"><a class="c9" href="https://www.google.com/url?q=http://cs231n.github.io/neural-networks-1/&amp;sa=D&amp;ust=1574279330599000">http://cs231n.github.io/neural-networks-1/</a></span></p><h4 class="c1" id="h.dhkwmbz4e6js"><span class="c0">Neural networks part 2</span></h4><p class="c3"><span class="c6">2.7 </span><span class="c5"><a class="c9" href="https://www.google.com/url?q=http://cs231n.github.io/neural-networks-2/&amp;sa=D&amp;ust=1574279330599000">http://cs231n.github.io/neural-networks-2/</a></span></p><h4 class="c1" id="h.h8r53vq8ynkt"><span class="c0">Neural networks part 3</span></h4><p class="c3"><span class="c6">2.8 </span><span class="c5"><a class="c9" href="https://www.google.com/url?q=http://cs231n.github.io/neural-networks-3/&amp;sa=D&amp;ust=1574279330600000">http://cs231n.github.io/neural-networks-3/</a></span></p><h3 class="c26" id="h.j3nb1l2cuezs"><span class="c23">Slides</span></h3><p class="c12"><span>Neural networks representation: non-linear hypotheses (Andrew Ng, ML wk 4 lecture 8)</span></p><h2 class="c8" id="h.x88b8s9lxwn5"><span class="c15">Additional resources</span></h2><h4 class="c34" id="h.s6sc9sy44g3e"><span class="c0">create your own neural network</span></h4><p class="c12"><span class="c4">playground.tensorflow.org</span></p><h4 class="c34" id="h.qxl6s05ast2l"><span class="c0">k-nearest neighbors demo</span></h4><p class="c12"><span class="c4">vision.stanford.edu</span></p><h4 class="c34" id="h.wv6enpe6t7r1"><span class="c0">Linear Classification Demo</span></h4><p class="c12"><span class="c5 c21"><a class="c9" href="https://www.google.com/url?q=http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/&amp;sa=D&amp;ust=1574279330602000">http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/</a></span></p><h4 class="c34" id="h.xybo2n6f742e"><span class="c0">Code Words (not sure how it&rsquo;s connected exactly)</span></h4><p class="c12"><span class="c4">codewords.recurse.com</span></p><p class="c12 c17"><span class="c4"></span></p><div><p class="c17 c30"><span class="c4"></span></p></div></body></html>